"""
    File for SAECC model

    Authors:
        Anon <anon@anon.anon>

    Date created: March 11, 2020
    Python version: 3.6.0

    # TODO: add index attribute to InstanceFeature
"""
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence

from torch.utils.data import DataLoader as TorchDataLoader

from module import SGCNConv

from ABSA.models import LCFS_BERT
from ABSA.data_utils import pad_or_truncate_tensorlist, text_to_berttok_seq
from ABSA.data_utils import retok_with_dist

from utils import dynamic_padding, pad_or_trunc_to_fixlength
from constants import *
from transformers import BertTokenizer, BertModel
from transformers.modeling_bert import BertPooler, BertSelfAttention, BertConfig

class SaeccModel(nn.Module):
    def __init__(self, args):
        self().__init__()
        self.cpc_pipeline = CpcPipeline(args)
        self.absa_pipeline = AbsaPipeline(args)

        hidden_dim = self.cpc_pipeline.output_dim + self.absa_pipeline.output_dim
        self.linear = nn.Linear(hidden_dim, 3)

        self._reset_params()

    def forward(self, batch):
        # TODO: Differentiate cpc and absa!
        hidden_cpc = self.cpc_pipeline(batch)
        hidden_absa = self.absa_pipeline(batch)

        hidden_agg = torch.cat(hidden_cpc.values())
        hidden_agg = torch.cat([hidden_agg, hidden_absa['logits']])
        
        logits = nn.linear(hidden_agg)
        return logits

    def _reset_params(self):
        initializer = torch.nn.init.xavier_normal
        for child in self.model.children():
            if type(child) == BertModel:  # skip bert params
                continue
            for p in child.parameters():
                if p.requires_grad:
                    if len(p.shape) > 1:
                        initializer(p)
                    else:
                        stdv = 1. / math.sqrt(p.shape[0])
                        torch.nn.init.uniform_(p, a=-stdv, b=stdv)


class AbsaPipeline(nn.Module):
    def __init__(self, args):
        super().__init__()

        """
        # TODO: argument to args
        self.batch_size = int(batch_size)
        self.absa = SAECC_ABSA(self.batch_size)

        # TODO: save output_dim for saecc
        self.output_dim = None
        """

        # === New code ===
        self.max_seq_len = args.absa_max_seq_len
        self.tokenizer = BertTokenizer.from_pretrained(args.bert_version)
        self.model = LCFS_BERT(args)

        # TODO: figure out what's done here.
        _params = filter(lambda p: p.requires_grad, self.model.parameters())


    def forward(self, batch):
        """
        batch_embedding: 
          a list of tensors, each of shape [sentence length, 768]
        batch_instance_feature: 
          a list of instance features that are in the same order as in batch_embedding
        """
        # TODO: move to the right place in the right format
        absa_batch = self.convert_batch_to_absa_batch(batch)

        # TODO: feature and logits
        features, logits = self.model(absa_batch)

        return features, logits # TODO: dimension of outputs
    
    def convert_batch_to_absa_batch(self, original_batch):
        emb = original_batch['embedding']
        ins_feat = original_batch['instance_feature']
        entities = [ins.get_entities()[0] for ins in ins_feat]

        assert len(emb) == len(ins_feat), "Num of emb doesn't match num of ins_feat"

        padded_emb = pad_or_truncate_tensorlist(emb, self.max_seq_len)  # (batch_size, absa_fix_len)
        
        # TODO: sentence or sentence_raw???
        # bert_tkn_text_idx_list: a list of **Fixed-Length** sentences
        bert_tkn_text_idx_list = [
            text_to_berttok_seq(self.tokenizer, ins.sentence, self.max_seq_len) for ins in ins_feat]
        bert_tkn_asp_idx_list = [
            text_to_berttok_seq(self.tokenizer, ent, self.max_seq_len) for ent in entities]
        # TODO: need that in tensor or list[list]

        # list of tuples: raw_tokens, dist
        # raw_tokens: tokens generated by spaCy
        # dist: distance to aspect term (if multiple aspect term, compute mean dist)
        pretoken_depdist_list = [
            calculate_dep_dist(ins.sentence, ins.get_entities()[0])
            for ins in ins_feat]
        
        # Add CLS/SEP to spacy.doc.token lists, add 0 to beginning & end of dis
        CLS, SEP = self.tokenizer.cls_token, self.tokenizer.sep_token
        pretoken_depdist_list = [([CLS]+tokens+[SEP], [0]+distances+[0]) 
            for tokens, distances in pretoken_depdist_list]

        token_dist_list = [retok_with_dist(self.tokenizer, x[0], x[1], self.max_seq_len) 
            for x in pretoken_depdist_list]
        
        # TODO: token_dist_list will later come from original_batch
        """
        # New way to get that will become:
        token_dist_list = original_batch["token_distance_list"]
        # TODO: pay attention that CPC will have two distance list
        """

        absa_batch = {
            "bert_embedding": padded_emb,
            "text_raw_bert_indices": bert_tkn_text_idx_list,
            "aspect_bert_indices": bert_tkn_asp_idx_list,
            "dep_distance_to_aspect": token_dist_list
        }
        return absa_batch



class CpcPipeline(nn.Module):
    def __init__(self, args):
        super().__init__()
        # global context
        sgcn_convs = []
        sgcn_dims = [args.embed_dim] + args.sgcn_dims
        self.sgcn_convs = [
            SGCNConv(
                dim_in=d_in,
                dim_out=d_out,
                num_labels=len(DEPENDENCY_LABELS),
                gating=args.sgcn_gating
            )
            for d_in, d_out in zip(sgcn_dims[:-1], sgcn_dims[1:])
        ]

        # local context
        self.lstm = nn.LSTM(
            input_size=args.embed_dim,
            hidden_size=args.hidden_dim,
            batch_first=True
        )

        self.output_dim = (args.hidden_dim + args.sgcn_dims) * 2

    def forward(self, batch):
        # global context
        depgraph = batch['depgraph']
        for conv in self.sgcn_convs:
            depgraph.x = conv(
                x=depgraph.x,
                edge_index=depgraph.edge_index,
                edge_label=depgraph.edge_attr
            )
        node_hidden = pad_sequence(
            [dg.x for dg in depgraph.to_data_list()],
            batch_first=True
        )

        # local context
        word_embedding = batch['embedding']
        word_hidden = self.lstm(word_embedding)[0]

        assert node_hidden.shape[0] == word_hidden.shape[0], 'batch size do not match'
        assert node_hidden.shape[1] == word_hidden.shape[1], 'seq_len do not match'

        nodeA, nodeB, wordA, wordB = self._extract_entities(batch, node_hidden, word_hidden)

        return {
            'nodeA': torch.cat(nodeA),
            'nodeB': torch.cat(nodeB),
            'wordA': torch.cat(wordA),
            'wordB': torch.cat(wordB)
        }

    def _extract_entities(self, batch, node_hidden, word_hidden):
        # extract entities
        instances = batch['instances']
        entA_pos = [torch.tensor(ins.entityA_pos) for ins in instances]
        entB_pos = [torch.tensor(ins.entityB_pos) for ins in instances]

        nodeA, nodeB = [], []
        for seq, posA, posB in zip(node_hidden, entA_pos, entB_pos):
            embedA = torch.index_select(seq, 0, posA)
            nodeA.append(torch.mean(embedA, dim=0))
            embedB = torch.index_select(seq, 0, posB)
            nodeB.append(torch.mean(embedB, dim=0))

        wordA, wordB = [], []
        for seq, posA, posB in zip(word_hidden, entA_pos, entB_pos):
            embedA = torch.index_select(seq, 0, posA)
            wordA.append(torch.mean(embedA, dim=0))
            embedB = torch.index_select(seq, 0, posB)
            wordB.append(torch.mean(embedB, dim=0))


def FastCpcPipeline(CpcPipeline):
    def _extract_entities(self, batch, node_hidden, word_hidden):
        # TODO: vectorize
        pass
